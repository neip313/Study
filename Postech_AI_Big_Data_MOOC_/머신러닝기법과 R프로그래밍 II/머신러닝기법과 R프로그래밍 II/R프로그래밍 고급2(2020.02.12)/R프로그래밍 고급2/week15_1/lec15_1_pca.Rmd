---
title: "lec15_1_pca"
author: "p_new_h"
date: '2020 11 28 '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# lec15_1_pca.r
# Multivariate analysis
# 주성분분석 Principle Component Analysis
##  다변량분석기법
 ‘주성분’이라고 불리는 선형조합으로 표현하는 기법
 여기서 주성분은 공분산(XTX)으로부터 eigenvector와 eigenvalue를 도출하여 계산됨

##• 주성분간의 수직관계

• 1st 주성분 (PC1) : 독립변수들의 변동(분산)을 가장 많이 설명하는 성분
• 2nd 주성분 (PC2) : PC1과 수직인 주성분
 (첫번째 주성분이 설명하지 못하는 변동에 대해 두번째로 설명하는 성분)

### set working directory
setwd("D:/tempstore/moocr/wk15")

• iris데이터(4개변수)의 주성분분석 – 차원축소 & 예측력 향상
```{r}
#input data(iris)

iris<-read.csv(file="iris.csv")
attach(iris)
head(iris)
```

```{r}
#Check correlation
# 독립변수간 상관관계 확인
cor(iris[1:4])
```

- 주성분분석을 위한 함수 : prcomp(독립변수들, center= , scale= )
 옵션을 주지않으면 center=T, scale=F  
- 주성분의 weight값을 제공한다.
 center=T, scale=T는 변수들의 평균을 빼고, 편차로 나누어 표준화한다는 의미.
```{r}
# 1.PCA(center=T->mean=0, scale.=T->variance=1)
ir.pca<-prcomp(iris[,1:4],center=T,scale.=T)
# PC1 = 0.5211*Sepal.Length -0.2693* Sepal.Width + 0.5804 * Petal.Length + 0.5649*Petal.Width
ir.pca
# 전체분산 중 각 주성분의 설명하는 비율

```

```{r}
summary(ir.pca)
#  PC1은 전체분산의 72.96%를 설명
#  PC2은 전체분산의 22.85%를 설명
#  PC3는 전체분산의 3.67%를 설명
#  PC4는 전체분산의 0.5%를 설명
# 누적설명비율을 보면 PC1와 PC2, 두개의 성분으로 전체분산의 95.81%를 설명
```
그러면 몇 개의 주성분으로
전체분산을 설명하는게 최적?


```{r}
# ir.pca is the weight to get 1st-4th principal compoenents 
# • 최적 주성분 수는? – scree plot을 그려보고 급격히 떨어지기 전까지의 PC를 선택
# 2.scree plot : to choose the number of components
plot(ir.pca,type="l")

```
 3rd PC에서 설명력이 급격하게 떨어짐을 볼 수 있음
 기울기가 꺾이는 PC3을 ‘elbow point’라 부름
 => 이 경우는 PC1, PC2 까지 사용하는 것을 추천


• screeplot함수를 이용 : screeplot(pca결과)
```{r}
# either way to draw scree plot
screeplot(ir.pca)
```
앞의 그림과 동일한 결과
=> PC1, PC2까지 사용 추천


• PC계산= X_data(n*p) %*% PCA_weight(p*p)
```{r}
# 3. calculate component=x_data%*% PCA weight
PRC<-as.matrix(iris[,1:4])%*%ir.pca$rotation
head(PRC)
```


```{r}
# 4. classification using principal components
# make data with components
iris.pc<-cbind(as.data.frame(PRC), Species)
head(iris.pc)
```

• 주성분을 이용한 서포트벡터머신 수행 (iris data)
### 5. support vector machine

```{r}
iris.pc$Species <- as.factor(iris.pc$spicies)
# install.packages("e1071")
library (e1071)
# classify all data using PC1-PC4 using support vector machine
m1<- svm(Species ~., data = iris.pc, kernel="linear")
# m2<- svm(Species ~PC1+PC2, data = iris.pc, kernel="linear")
summary(m1)
```

PC1-PC4까지 모두를 input으로
분류모형(서포트벡터머신) 수행



```{r}
# predict class for all data 
x<-iris.pc[, -5]
pred <- predict(m1, x)
# check accuracy between true class and predicted class
y<-iris.pc[,5]
table(pred, y)
```




