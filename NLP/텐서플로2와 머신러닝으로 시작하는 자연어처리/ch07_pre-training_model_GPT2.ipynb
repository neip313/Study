{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch07. 사전 학습 모델(GPT)\n",
    "## 03. GPT\n",
    "\n",
    "### 1) GPT 1\n",
    "- GPT(Generative Pre-training)1\n",
    "  - 큰 자연어 처리 데이터를 비지도 학습으로 사전 학습한 후, 학습된 가중치로 풀고자 하는 문제를 미세 조정하는 방법론의 모델\n",
    "\n",
    "\n",
    "- 모델 구조\n",
    "  - 트랜스포머 모델 사용(버트와 동일)\n",
    "  - 단, GPT1에서는 트랜스포머의 디코더 구조만 사용(버트는 인코더 구조만 사용)\n",
    "\n",
    "\n",
    "- 사전 학습\n",
    "  - 버트와 달리 하나의 사전 학습 방식(전통적 언어 모델 방식) 사용\n",
    "  - 앞 단어를 활용해 다음 단어를 예측하는 방식으로 사전 학습 진행\n",
    "  - 별도 라벨이 존재하지 않는 데이터도 학습 가능\n",
    "    - 비지도 학습으로 분류\n",
    "  - 많은 데이터로 모델 가중치를 사전 학습할 수 있음\n",
    "  - 버트와 달리, 실제 문제 대상으로 학습 진행 시에도 언어 모델을 함께 학습\n",
    "  \n",
    "input|label\n",
    "---|---\n",
    "\"< START>\"|\"나는\"\n",
    "\"< START>\",\"나는\"|\"학교에\"\n",
    "\"< START>\",\"나는\",\"학교에\"|\"간다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) GPT 2\n",
    "- GPT(Generative Pre-training)2\n",
    "  - OpenAI 제안 , 2018년 발표된 GPT1 모델의 성능을 향상한 모델로서 텍스트 생성에서 특히 좋은 성능을 보임\n",
    "  \n",
    "  \n",
    "- 모델 구조\n",
    "  - 대부분 GPT1과 동일, 트랜스포머의 디코더를 기반으로 하는 모델\n",
    "  - 차이점: 레이어 노멀라이제이션이 각 부분 블록의 입력 쪽으로 이동 (기존에는 각 레이어 직후 레지듀얼 커넥션과 함께 적용)\n",
    "\n",
    "\n",
    "- 학습 데이터 및 모델 크기\n",
    "\n",
    "| |GPT1|GPT2|\n",
    "|---|---|---|\n",
    "|레이어|12개|117만 개|\n",
    "|가중치|48개|1,542만 개|\n",
    "\n",
    "\n",
    "- 모델 입력\n",
    "  - BPE(Byte Pair Encoding) 방식을 사용해 텍스트를 나누어 입력값으로 사용\n",
    "  - 글자와 문자 사이 적절한 단위를 찾아 나누는 방식으로 높은 성능을 보임\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
